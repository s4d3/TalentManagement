# -*- coding: utf-8 -*-
"""01.b_SDW_TM_Final_32F_BalanceDataTrain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_g9GCnm-8ujG3qQARxxOOn07kr7s8lIC

# B. Training dengan 32 features

## Koneksi GDrive
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""## Import Library"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, f1_score
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SelectFromModel

"""## Load Dataset"""

# Load dataset
dataset_path = "/content/drive/MyDrive/02_TelU/01_KK-AIS/2023 - Talent Management/04_Colab/df_renamed.csv"  # Ganti dengan path yang sesuai
df = pd.read_csv(dataset_path)

df

"""## Pemilihan Fitur dan Label"""

# Pilih kolom secara eksplisit sesuai daftar fitur yang diberikan
selected_features = [
    'MD', 'MI', 'MS', 'MC', 'KM', 'DA', 'KN', 'KV', 'LB', 'MB',  # DISC + Soft Skill
    # tambahkan fitur lainnya sesuai daftar
    'LD', 'LI', 'LS', 'CD', 'CI', 'CS', 'CC',
    'L', 'I', 'T', 'V', 'X', 'B', 'O', 'R', 'D', 'C', 'Z', 'E', 'K', 'F', 'W'
]

# Definisikan X dan y sesuai kolom yang dipilih
X = df[selected_features]
y = df["Label"]

# Tampilkan hasilnya untuk mengecek
print("X shape:", X.shape)
print("Selected features:\n", X.columns.tolist())
# print("\n y Labels:")
# print(y.value_counts())

"""## Label Encoder"""

# Encode label
le = LabelEncoder()
y_encoded = le.fit_transform(y)

"""## Feature Scaling"""

# Normalisasi fitur
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled

df_lengkap = pd.DataFrame(X_scaled)
df_lengkap

# prompt: add y_encoded to df_lengkap

df_lengkap['Label'] = y_encoded
df_lengkap

"""## Pengecekan Data Imbalanced"""

df['Label'].value_counts()

df_lengkap['Label'].value_counts()

import matplotlib.pyplot as plt

# Plot distribusi kelas
df['Label'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')

plt.xlabel("Class Labels")
plt.ylabel("Count")
plt.title("Class Distribution")
plt.xticks(rotation=0)  # Agar label tetap horizontal
plt.show()

import matplotlib.pyplot as plt

# Plot distribusi kelas
df_lengkap['Label'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')

plt.xlabel("Class Labels")
plt.ylabel("Count")
plt.title("Class Distribution")
plt.xticks(rotation=0)  # Agar label tetap horizontal
plt.show()

"""## Balancing With SMOTE"""

# Manual split dulu
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.3, random_state=0, stratify=y_encoded)

# Terapkan SMOTE hanya pada data training
smote = SMOTE(random_state=0, k_neighbors=1)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Plot distribusi setelah SMOTE
pd.Series(y_train_resampled).value_counts().plot(kind='bar', color='skyblue')
plt.title('Class Distribution After SMOTE (Training Data)')
plt.xlabel('Class Labels')
plt.ylabel('Frequency')
plt.show()

# prompt: how to save X_train_resampled and y_train_resampled into dataframe

import pandas as pd
df_X_train_resampled = pd.DataFrame(X_train_resampled, columns=X.columns)
df_y_train_resampled = pd.DataFrame(y_train_resampled, columns=['Label'])

# Gabungkan X dan y ke dalam satu DataFrame
df_train_resampled = pd.concat([df_X_train_resampled, df_y_train_resampled], axis=1)

# Tampilkan contoh hasil
df_train_resampled.head()

df_train_resampled['Label'].value_counts()
#df_lengkap['Label'].value_counts()

"""## Training the model

Pada tahap ini dilakukan training dengan menggunakan 5 model, yaitu: RandomForest, SVM, KNN, XGBoost dan Neural Network
"""

# Model sebelum HPO
models = {
    "Random Forest": RandomForestClassifier(n_estimators=300, max_depth=15, random_state=0),
    "SVM": SVC(kernel='rbf', C=5, gamma='scale', random_state=0),
    "KNN": KNeighborsClassifier(n_neighbors=5, weights='distance'),
    "XGBoost": XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=10, random_state=0, eval_metric="mlogloss")
}

train_accuracies, test_accuracies, f1_scores, predictions = [], [], [], {}

# Training models (selain Neural Network)
for name, model in models.items():
    model.fit(X_train_resampled, y_train_resampled)
    y_pred = model.predict(X_test)

    train_acc = accuracy_score(y_train_resampled, model.predict(X_train_resampled))
    test_acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')

    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)
    f1_scores.append(f1_score(y_test, y_pred, average='weighted'))
    predictions[name] = y_pred

# Neural Network Sebelum HPO
input_dim = X_train_resampled.shape[1]

mlp_model = Sequential([
    Input(shape=(input_dim,)),
    Dense(128, activation='relu'),
    Dropout(0.4),
    Dense(64, activation='relu'),
    Dense(len(np.unique(y_train_resampled)), activation='softmax')
])

mlp_model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

mlp_model.fit(X_train_resampled, y_train_resampled, epochs=30, batch_size=32, verbose=0)

# Prediction dan Evaluasi Neural Network
y_pred_mlp = mlp_model.predict(X_test).argmax(axis=1)

train_accuracies.append(accuracy_score(y_train_resampled, mlp_model.predict(X_train_resampled).argmax(axis=1)))
test_accuracies.append(accuracy_score(y_test, y_pred_mlp))
f1_scores.append(f1_score(y_test, y_pred_mlp, average='weighted'))
predictions["Neural Network"] = y_pred_mlp

# Simpan hasil sebelum HPO
results_before_hpo = {
    "train_accuracies": train_accuracies,
    "test_accuracies": test_accuracies,
    "f1_scores": f1_scores,
    "predictions": predictions
}
pd.to_pickle(results_before_hpo, "results_before_hpo.pkl")

# Simpan data yang digunakan
pd.to_pickle((X_train_resampled, X_test, y_train_resampled, y_test), "data_split.pkl")

print("Training sebelum HPO selesai. Hasil disimpan di results_before_hpo.pkl")

"""### Pengecekan overfitting"""

import pandas as pd

# Hitung Train-Test Gap
train_test_gap = [train - test for train, test in zip(train_accuracies, test_accuracies)]

# Buat kolom overfitting indication
def overfitting_status(gap):
    if gap <= 0.15:
        return "Not Overfitting"
    elif gap <= 0.30:
        return "Possible Overfitting"
    else:
        return "Overfitting"

overfitting_indication = [overfitting_status(gap) for gap in train_test_gap]


# Assuming you have the train_accuracies and test_accuracies lists from your code
model_names = ["Random Forest", "SVM", "KNN", "XGBoost", "Neural Network"]
data = {  # Remove extra indentation here
    "Model": model_names,
    "Train Accuracy": train_accuracies,
    "Test Accuracy": test_accuracies,
    # "Train-Test Gap": [train_acc - test_acc for train_acc, test_acc in zip(train_accuracies, test_accuracies)]
    "Train-Test Gap": train_test_gap,
    "Overfitting Indication": overfitting_indication
}

df_results = pd.DataFrame(data)
df_results

"""### Classification Report"""

from sklearn.metrics import classification_report

for name, y_pred in predictions.items():
    print(f"\n=== Classification Report for {name} ===")
    print(classification_report(y_test, y_pred))

from sklearn.metrics import classification_report

# Tampilkan laporan klasifikasi untuk semua model
for name, y_pred in predictions.items():
    print(f"=== Classification Report for {name} ===")
    print(classification_report(y_test, y_pred))
    print("\n" + "="*50 + "\n")

"""### Confusion Matrix"""

from sklearn.metrics import ConfusionMatrixDisplay

for name, y_pred in predictions.items():
    print(f"\n=== Confusion Matrix for {name} ===")
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
    plt.title(f"Confusion Matrix - {name}")
    plt.show()

from sklearn.metrics import ConfusionMatrixDisplay

for name, y_pred in predictions.items():
    print(f"\n=== Confusion Matrix for {name} ===")
    ConfusionMatrixDisplay.from_predictions(
        y_test, y_pred,
        display_labels=le.classes_,
        cmap='Blues'
    )
    plt.title(f"Confusion Matrix - {name}")
    plt.xticks(rotation=45)
    plt.tight_layout()

"""Code ini digunakan untuk membandingkan performa model dengan dua jenis evaluasi berbeda, yaitu Macro F1 Score dan Weighted F1 Score, menggunakan Cross Validation (CV) yang dikombinasikan dengan SMOTE di dalam pipeline.

### Pengecekan Macro F1 Scores dan Weighted F1 Scores
"""

from imblearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer, f1_score
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Macro F1 Score
macro_scorer = make_scorer(f1_score, average='macro')
pipeline_macro = Pipeline([
    ('smote', SMOTE(random_state=0, k_neighbors=1)),
    ('rf', RandomForestClassifier(random_state=0))
])

macro_scores = cross_val_score(pipeline_macro, X_scaled, y_encoded, cv=5, scoring=macro_scorer)
print("Macro F1 Scores:", macro_scores)
print("Average Macro F1 Score:", np.mean(macro_scores))

print("\n" + "="*60 + "\n")

# Weighted F1 Score
weighted_scorer = make_scorer(f1_score, average='weighted')
pipeline_weighted = Pipeline([
    ('smote', SMOTE(random_state=0, k_neighbors=1)),
    ('rf', RandomForestClassifier(random_state=0))
])

weighted_scores = cross_val_score(pipeline_weighted, X_scaled, y_encoded, cv=5, scoring=weighted_scorer)
print("Weighted F1 Scores:", weighted_scores)
print("Average Weighted F1 Score:", np.mean(weighted_scores))

"""Macro F1 Scores: [0.111, 0.111, 0.111, 0.111, 0.112]
Average Macro F1 Score: 0.1114



*   Nilainya sangat rendah (~0.11).
*   Artinya: rata-rata performa prediksi per kelas sangat rendah, terutama karena kelas minoritas tidak diprediksi dengan baik sama sekali.
* Karena macro F1 memberi bobot yang sama untuk setiap kelas, maka model kamu terlihat sangat lemah untuk kelas-kelas kecil (minoritas).

Kesimpulan: Model kamu hanya bagus di kelas mayoritas, dan gagal mengenali kelas minoritas dengan baik.

Weighted F1 Scores: [0.715, 0.717, 0.718, 0.718, 0.725]
Average Weighted F1 Score: 0.7185



*   Nilainya cukup tinggi (~0.72).
*   Karena ini adalah rata-rata F1 Score dengan mempertimbangkan proporsi jumlah kelas, maka skor yang tinggi ini didominasi oleh kelas mayoritas (misalnya kelas HA atau HRP jika jumlahnya paling besar).
* Jadi, walaupun prediksi minoritas buruk, skor tetap tinggi karena kelas mayoritas sangat banyak dan diprediksi baik.

Kesimpulan Umum

Aspek	Nilai	Artinya
Macro F1 Score	~0.11	Model tidak adil terhadap semua kelas, performa di kelas minoritas sangat rendah.

Weighted F1 Score	~0.72	Model terlihat bagus, tapi hanya karena mendominasi di kelas mayoritas.

###  Visualisasi (F1 Scores, Accuracy)
"""

# Visualisasi sebelum HPO
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

# Load hasil sebelum HPO
results_before_hpo = pd.read_pickle("results_before_hpo.pkl")
X_train, X_test, y_train, y_test = pd.read_pickle("data_split.pkl")

# Ambil data
train_accuracies = results_before_hpo["train_accuracies"]
test_accuracies = results_before_hpo["test_accuracies"]
f1_scores = results_before_hpo["f1_scores"]
predictions = results_before_hpo["predictions"]

model_names = list(predictions.keys())

# 1Ô∏è‚É£ Visualisasi F1 Score Sebelum HPO
plt.figure(figsize=(10, 5))
plt.bar(model_names, f1_scores, color='red', alpha=0.7, label='Before HPO')
plt.ylabel("F1 Score")
plt.title("F1 Scores Before HPO")
plt.legend()
plt.show()

# 2Ô∏è‚É£ Visualisasi Confusion Matrix Sebelum HPO
plt.figure(figsize=(12, 10))
for i, (name, y_pred) in enumerate(predictions.items()):
    plt.subplot(3, 2, i + 1)
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=np.unique(y_test), cmap="Blues", ax=plt.gca())
    plt.title(f"{name} - Before HPO")
plt.tight_layout()
plt.show()

# 3Ô∏è‚É£ Visualisasi Train vs Test Accuracy Sebelum HPO
plt.figure(figsize=(12, 6))
x = np.arange(len(model_names))
plt.bar(x - 0.2, train_accuracies, width=0.4, label='Train Accuracy', color='blue')
plt.bar(x + 0.2, test_accuracies, width=0.4, label='Test Accuracy', color='red')
plt.xticks(x, model_names, rotation=15)
plt.ylabel("Accuracy Score")
plt.title("Train & Test Accuracy Before HPO")
plt.legend()
plt.show()

print("Visualisasi sebelum HPO selesai.")

# prompt: I want to show example of real data (label) and the prediction from each models

import pandas as pd
# Assuming 'predictions' dictionary from the previous code holds predictions for each model
# and y_test contains the true labels.
# Also, assuming 'model_names' is defined as in the original code


# Number of samples to display
num_samples_to_display = 10  # You can adjust this

# Create a DataFrame to hold the results
comparison_df = pd.DataFrame({'Real Label': le.inverse_transform(y_test[:num_samples_to_display])})


for model_name in model_names:
    predicted_labels = le.inverse_transform(predictions[model_name][:num_samples_to_display])
    comparison_df[model_name] = predicted_labels

comparison_df

"""semua prediksi model tampak seragam (HA semua) atau hanya sedikit berbeda, padahal seharusnya bisa lebih bervariasi. Ini sebenarnya indikator penting bahwa:
1. Masalah: Model overfitting atau bias pada kelas mayoritas (HA)

Artinya model kamu cenderung menebak mayoritas sebagai HA, bahkan ketika real label-nya bukan HA (seperti baris ke-6 dan ke-7), prediksi tetap HA.

Kemungkinan penyebab utama:

* Test set tidak seimbang ‚Üí Kelas minoritas (SSP, VD, HRP, HD) terlalu sedikit, sehingga model tidak belajar cukup dari kelas tersebut.
* SMOTE hanya di training set ‚Üí Ini memang benar secara praktik umum, tapi kalau distribusi label di test set tidak mencerminkan yang seimbang, akurasi terhadap kelas minoritas tetap rendah.
* Model bias terhadap kelas mayoritas karena F1-score weighted masih memberikan bobot besar pada HA (kelas mayoritas).

Solusi & Analisis Lanjutan:
* Cek distribusi label di test set:

python
Copy code
pd.Series(le.inverse_transform(y_test)).value_counts()

* Lihat Confusion Matrix ‚Üí kemungkinan besar prediksi terkonsentrasi di satu kolom (HA), yang artinya model prediksi sangat tidak seimbang.

* Gunakan Macro F1 score dan perhatikan klasifikasi per label ‚Üí Macro F1 score akan menunjukkan rata-rata F1 tiap label, sehingga tidak bias ke kelas mayoritas.

* Tampilkan tabel prediksi + highlight kesalahan ‚Üí Biar lebih jelas.

## Model setelah HPO
"""

# Model setelah HPO

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Load hasil sebelum HPO
results_before_hpo = pd.read_pickle("results_before_hpo.pkl")
X_train, X_test, y_train, y_test = pd.read_pickle("data_split.pkl")

# Inisialisasi ulang model untuk GridSearchCV
models = {
    "Random Forest": RandomForestClassifier(random_state=0),
    "SVM": SVC(random_state=0),
    "KNN": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(random_state=0, eval_metric="mlogloss")
}

# Hyperparameter Optimization dengan GridSearchCV
param_grid = {
    "Random Forest": {'n_estimators': [100, 200, 300], 'max_depth': [10, 15, 20]},
    "SVM": {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},
    "KNN": {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']},
    "XGBoost": {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [5, 10]}
}

optimized_train_accuracies, optimized_test_accuracies, optimized_f1_scores = [], [], []
optimized_models = {}

for name, model in models.items():
    grid_search = GridSearchCV(model, param_grid[name], cv=5, scoring='accuracy', verbose=1)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    optimized_models[name] = best_model
    y_pred_optimized = best_model.predict(X_test)
    optimized_train_accuracies.append(accuracy_score(y_train, best_model.predict(X_train)))
    optimized_test_accuracies.append(accuracy_score(y_test, y_pred_optimized))
    optimized_f1_scores.append(f1_score(y_test, y_pred_optimized, average='weighted'))

# Optimasi Neural Network secara manual
mlp_hpo_model = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(256, activation='relu'),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dense(len(np.unique(y_train)), activation='softmax')
])
mlp_hpo_model.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
mlp_hpo_model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=0)
y_pred_mlp_hpo = mlp_hpo_model.predict(X_test).argmax(axis=1)
nn_f1_score = f1_score(y_test, y_pred_mlp_hpo, average='weighted')
nn_train_accuracy = accuracy_score(y_train, mlp_hpo_model.predict(X_train).argmax(axis=1))
nn_test_accuracy = accuracy_score(y_test, y_pred_mlp_hpo)

# Simpan hasil setelah HPO
results_after_hpo = {
    "optimized_train_accuracies": optimized_train_accuracies,
    "optimized_test_accuracies": optimized_test_accuracies,
    "optimized_f1_scores": optimized_f1_scores,
    "optimized_models": optimized_models,
    "nn_f1_score": nn_f1_score,
    "nn_train_accuracy": nn_train_accuracy,
    "nn_test_accuracy": nn_test_accuracy,
    "nn_predictions": y_pred_mlp_hpo
}
pd.to_pickle(results_after_hpo, "results_after_hpo.pkl")

print("Training setelah HPO selesai. Hasil disimpan di results_after_hpo.pkl")
#10m

"""### Pengecekan overfitting"""

import pandas as pd

# Ambil hasil dari results_after_hpo
results_after_hpo = pd.read_pickle("results_after_hpo.pkl")

# Ambil data akurasi & F1-score
optimized_train_accuracies = results_after_hpo["optimized_train_accuracies"]
optimized_test_accuracies = results_after_hpo["optimized_test_accuracies"]
optimized_f1_scores = results_after_hpo["optimized_f1_scores"]

# Tambahkan Neural Network (jika disimpan secara terpisah)
optimized_train_accuracies.append(results_after_hpo["nn_train_accuracy"])
optimized_test_accuracies.append(results_after_hpo["nn_test_accuracy"])
optimized_f1_scores.append(results_after_hpo["nn_f1_score"])

# Model Names
model_names = list(results_after_hpo["optimized_models"].keys()) + ["Neural Network"]

# Hitung Train-Test Gap
train_test_gap = [train - test for train, test in zip(optimized_train_accuracies, optimized_test_accuracies)]

# Menentukan indikasi overfitting
def check_overfitting(gap):
    if gap <= 0.15:
        return "Not Overfitting"
    elif gap <= 0.30:
        return "Possible Overfitting"
    else:
        return "Overfitting"

overfitting_indication = [check_overfitting(gap) for gap in train_test_gap]

# Buat DataFrame
df_eval_after_hpo = pd.DataFrame({
    "Model": model_names,
    "Train Accuracy": optimized_train_accuracies,
    "Test Accuracy": optimized_test_accuracies,
    "Train-Test Gap": train_test_gap,
    "Overfitting Indication": overfitting_indication
})

# Tampilkan hasil evaluasi
df_eval_after_hpo

"""### Classification Report"""

# Menyimpan hasil prediksi setelah HPO
optimized_predictions = {}

for name, model in optimized_models.items():
    y_pred_optimized = model.predict(X_test)
    optimized_predictions[name] = y_pred_optimized

# Tambahkan hasil prediksi Neural Network yang sudah dioptimasi
optimized_predictions["Neural Network"] = y_pred_mlp_hpo

# Cetak classification report semua model setelah HPO
from sklearn.metrics import classification_report

for name, y_pred in optimized_predictions.items():
    print(f"\n=== Classification Report for {name} (After HPO) ===")
    print(classification_report(y_test, y_pred))

# prompt: Show the best parameter from each model

import pandas as pd

# Load hasil setelah HPO
results_after_hpo = pd.read_pickle("results_after_hpo.pkl")

# Access the optimized models
optimized_models = results_after_hpo["optimized_models"]

# Print the best parameters for each model
for model_name, model in optimized_models.items():
    print(f"Model: {model_name}")
    print(f"Best parameters: {model.get_params()}")
    print("-" * 20)

# Print best parameters for Neural Network (optimized manually)
print("Model: Neural Network")
print("Best parameters (manually tuned):  Refer to the model definition in the code.") #Since it's manually optimized, there isn't a 'best_estimator_' or direct access to the parameter grid.

"""Best parameter for each models
- RandomForest:
  - n_estimators: 300 ==> 300
  - max_depth: 15 ==> 20
- SVM
  - C: 10 ==> 10
  - Kernel: rbf ==> rbf
- KNN
 - n_neighbors: 3 ==> 3
 - weights: distance ==> uniform
- XGBoost
 - n_estimators: 200 ==> 200
 - learning_rate: 0.1 ==> 0.1
 - max_depth: 5 ==> 10
- NN
 - Lihat di setting atas

### Pengecekan Macro F1 Scores dan Weighted F1 Scores
"""

from sklearn.metrics import f1_score

# Siapkan list kosong
macro_f1_scores = []
weighted_f1_scores = []

# Perhitungan untuk semua model hasil HPO
for name, model in results_after_hpo["optimized_models"].items():
    y_pred = model.predict(X_test)
    macro_f1 = f1_score(y_test, y_pred, average='macro')
    weighted_f1 = f1_score(y_test, y_pred, average='weighted')
    macro_f1_scores.append(macro_f1)
    weighted_f1_scores.append(weighted_f1)

# Tambahkan Neural Network secara manual
macro_f1_nn = f1_score(y_test, results_after_hpo["nn_predictions"], average='macro')
weighted_f1_nn = results_after_hpo["nn_f1_score"]  # sudah dihitung sebelumnya

macro_f1_scores.append(macro_f1_nn)
weighted_f1_scores.append(weighted_f1_nn)

# Gabungkan dalam DataFrame
model_names = list(results_after_hpo["optimized_models"].keys()) + ["Neural Network"]

df_f1_after_hpo = pd.DataFrame({
    "Model": model_names,
    "Macro F1 Score": macro_f1_scores,
    "Weighted F1 Score": weighted_f1_scores
})

# Tampilkan
df_f1_after_hpo

"""Makna dari masing-masing metrik:

1Ô∏è‚É£ Macro F1 Score:
Rata-rata F1 score dari semua kelas, tanpa mempertimbangkan jumlah sampel per kelas (semua kelas dianggap sama penting).
Nilai yang rendah menunjukkan bahwa model masih belum cukup baik dalam mengenali kelas minoritas.
Neural Network dan XGBoost memiliki nilai macro F1 tertinggi, artinya sedikit lebih baik dalam memperhatikan semua kelas secara seimbang.

2Ô∏è‚É£ Weighted F1 Score:
Menghitung F1 score rata-rata dengan mempertimbangkan proporsi kelas.
Karena dataset masih imbalanced, nilai weighted F1 cenderung tinggi karena dominasi kelas mayoritas (HA).
Hampir semua model (kecuali KNN) memiliki nilai weighted F1 > 0.71, tapi ini bisa menyesatkan karena performa terhadap kelas minoritas masih rendah (terlihat dari rendahnya Macro F1).

üìå Kesimpulan Utama:

Model seolah-olah terlihat bagus (weighted F1 tinggi), tetapi sebenarnya masih buruk terhadap kelas minoritas (macro F1 rendah).
Neural Network dan XGBoost sedikit lebih baik secara seimbang, tetapi secara keseluruhan klasifikasi terhadap kelas kecil tetap menjadi tantangan.

KNN paling buruk performanya (terlihat dari weighted F1 dan macro F1 terendah), meski sudah HPO.

‚ú® Rekomendasi:

Macro F1 Score sebaiknya digunakan sebagai metrik utama dalam imbalanced data, karena memberikan gambaran yang lebih adil.
Pertimbangkan metode lanjutan di masa depan: class weights, threshold tuning, atau focal loss (untuk NN) jika ingin meningkatkan sensitivitas terhadap kelas minoritas.

### Visualisasi
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

# Load hasil setelah HPO
results_after_hpo = pd.read_pickle("results_after_hpo.pkl")
X_train, X_test, y_train, y_test = pd.read_pickle("data_split.pkl")

# Ambil data
optimized_train_accuracies = np.array(results_after_hpo.get("optimized_train_accuracies", []))
optimized_test_accuracies = np.array(results_after_hpo.get("optimized_test_accuracies", []))
optimized_f1_scores = np.array(results_after_hpo.get("optimized_f1_scores", []))
optimized_models = results_after_hpo.get("optimized_models", {})

# Tambahkan Neural Network jika ada hasilnya
nn_f1 = results_after_hpo.get("nn_f1_score", None)
nn_train_acc = results_after_hpo.get("nn_train_accuracy", None)
nn_test_acc = results_after_hpo.get("nn_test_accuracy", None)
nn_predictions = results_after_hpo.get("nn_predictions", None)

if nn_f1 is not None and nn_train_acc is not None and nn_test_acc is not None and nn_predictions is not None:
    optimized_f1_scores = np.append(optimized_f1_scores, nn_f1)
    optimized_train_accuracies = np.append(optimized_train_accuracies, nn_train_acc)
    optimized_test_accuracies = np.append(optimized_test_accuracies, nn_test_acc)
    optimized_models["Neural Network"] = None  # Placeholder
else:
    print("Warning: Neural Network results missing. Ensure NN results are stored correctly.")

model_names = list(optimized_models.keys())

# Pastikan jumlah elemen sesuai
if len(optimized_f1_scores) != len(model_names):
    print("Warning: Mismatch between model names and F1 scores. Adjusting...")
    min_length = min(len(optimized_f1_scores), len(model_names))
    optimized_f1_scores = optimized_f1_scores[:min_length]
    model_names = model_names[:min_length]

# 1Ô∏è‚É£ Visualisasi F1 Score Setelah HPO
plt.figure(figsize=(10, 5))
plt.bar(model_names, optimized_f1_scores, color='green', alpha=0.7, label='After HPO')
plt.ylabel("F1 Score")
plt.title("F1 Scores After HPO")
plt.xticks(rotation=15)
plt.legend()
plt.show()

# 2Ô∏è‚É£ Visualisasi Confusion Matrix Setelah HPO
plt.figure(figsize=(12, 10))
for i, (name, model) in enumerate(optimized_models.items()):
    if model is not None:
        y_pred_optimized = model.predict(X_test)
    else:
        y_pred_optimized = nn_predictions if nn_predictions is not None else np.zeros_like(y_test)
    plt.subplot(3, 2, i + 1)
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred_optimized, display_labels=np.unique(y_test), cmap="Greens", ax=plt.gca())
    plt.title(f"{name} - After HPO")
plt.tight_layout()
plt.show()

# 3Ô∏è‚É£ Visualisasi Train vs Test Accuracy Setelah HPO
plt.figure(figsize=(12, 6))
x = np.arange(len(model_names))
plt.bar(x - 0.2, optimized_train_accuracies[:len(model_names)], width=0.4, label='Train Accuracy', color='lightblue')
plt.bar(x + 0.2, optimized_test_accuracies[:len(model_names)], width=0.4, label='Test Accuracy', color='pink')
plt.xticks(x, model_names, rotation=15)
plt.ylabel("Accuracy Score")
plt.title("Train & Test Accuracy After HPO")
plt.legend()
plt.show()

print("Visualisasi setelah HPO selesai.")

# prompt: Show examples of real label and prediction label from each models : Random Forest, SVM, KNN, XGBoost, and NN

import pandas as pd
import numpy as np

# Load hasil sebelum HPO dan data split
results_before_hpo = pd.read_pickle("results_before_hpo.pkl")
predictions = results_before_hpo["predictions"]
X_train, X_test, y_train, y_test = pd.read_pickle("data_split.pkl")

# Load label encoder
from sklearn.preprocessing import LabelEncoder  # Make sure LabelEncoder is imported
le = LabelEncoder()

# Fit the LabelEncoder with your original labels
# Assuming 'y' contains your original labels (from the original df['Label'])
# If 'y' is not available, you need to load or reconstruct it
# Replace 'df['Label']' with the correct variable if needed
le.fit(df['Label']) # fitting the label encoder with the original labels

# Number of samples to display
num_samples_to_display = 10

# Create a DataFrame to hold the results
model_names = list(predictions.keys())
comparison_df = pd.DataFrame({'Real Label': le.inverse_transform(y_test[:num_samples_to_display])})

for model_name in model_names:
    predicted_labels = le.inverse_transform(predictions[model_name][:num_samples_to_display])
    comparison_df[model_name] = predicted_labels

comparison_df

"""## Training model dengan class weight"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight
from sklearn.metrics import accuracy_score, f1_score, classification_report, ConfusionMatrixDisplay
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam

# --- Load Dataset ---
df = pd.read_csv("/content/drive/MyDrive/02_TelU/01_KK-AIS/2023 - Talent Management/04_Colab/df_renamed.csv")

# --- Feature Selection ---
selected_features = [
    'MD', 'MI', 'MS', 'MC', 'KM', 'DA', 'KN', 'KV', 'LB', 'MB',
    'LD', 'LI', 'LS', 'CD', 'CI', 'CS', 'CC',
    'L', 'I', 'T', 'V', 'X', 'B', 'O', 'R', 'D', 'C', 'Z', 'E', 'K', 'F', 'W'
]
X = df[selected_features]
y = df['Label']

# --- Label Encoding & Scaling ---
le = LabelEncoder()
y_encoded = le.fit_transform(y)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# --- Train-Test Split ---
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42)

# --- Class Weight Calculation ---
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))
sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)

# --- Model Training with Class Weights ---
models = {
    "Random Forest": RandomForestClassifier(n_estimators=300, max_depth=15, class_weight='balanced', random_state=42),
    "SVM": SVC(kernel='rbf', C=5, class_weight='balanced', probability=True, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5, weights='distance'),  # KNN doesn't support class_weight
    "XGBoost": XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=10, random_state=42, eval_metric='mlogloss')
}

train_accuracies, test_accuracies, f1_scores, predictions = [], [], [], {}

for name, model in models.items():
    if name == "XGBoost":
        model.fit(X_train, y_train, sample_weight=sample_weights)
    else:
        model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    predictions[name] = y_pred
    train_accuracies.append(accuracy_score(y_train, model.predict(X_train)))
    test_accuracies.append(accuracy_score(y_test, y_pred))
    f1_scores.append(f1_score(y_test, y_pred, average='weighted'))

# --- Neural Network with Class Weight ---
input_dim = X_train.shape[1]
mlp_model = Sequential([
    Input(shape=(input_dim,)),
    Dense(128, activation='relu'),
    Dropout(0.4),
    Dense(64, activation='relu'),
    Dense(len(np.unique(y_train)), activation='softmax')
])
mlp_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
mlp_model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=0, class_weight=class_weights_dict)

y_pred_mlp = mlp_model.predict(X_test).argmax(axis=1)
predictions["Neural Network"] = y_pred_mlp
train_accuracies.append(accuracy_score(y_train, mlp_model.predict(X_train).argmax(axis=1)))
test_accuracies.append(accuracy_score(y_test, y_pred_mlp))
f1_scores.append(f1_score(y_test, y_pred_mlp, average='weighted'))

# --- Evaluation Results ---
df_results = pd.DataFrame({
    "Model": list(predictions.keys()),
    "Train Accuracy": train_accuracies,
    "Test Accuracy": test_accuracies,
    "Train-Test Gap": [tr - te for tr, te in zip(train_accuracies, test_accuracies)],
    "F1 Score (Weighted)": f1_scores
})

print("\n=== Model Evaluation Summary ===")
print(df_results)

# --- Classification Report & Confusion Matrix ---
from sklearn.metrics import classification_report
for name, y_pred in predictions.items():
    print(f"\n=== Classification Report for {name} ===")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

from sklearn.metrics import ConfusionMatrixDisplay
for name, y_pred in predictions.items():
    print(f"\n=== Confusion Matrix for {name} ===")
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=le.classes_, xticks_rotation=45)
    plt.title(f"Confusion Matrix - {name}")
    plt.show()